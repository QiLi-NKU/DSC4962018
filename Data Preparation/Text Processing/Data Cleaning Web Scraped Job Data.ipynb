{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning Web Scraped Job Data\n",
    "In approaching the task of cleaning my data, I had a few objectives:\n",
    "1. eliminate all duplicate job postings from my data\n",
    "2. format all available salary data so that salaries were presented in terms of a single figure detailing expected annual compensation\n",
    "3. conduct any initial cleaning of string data that will make future natural language processing easier when building my predictive models\n",
    "\n",
    "https://towardsdatascience.com/data-cleaning-web-scraped-job-data-6c2a2d963cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#reading csv with index_col = 0, otherwise I get an additional\n",
    "#unnamed column of separate index values\n",
    "scrape_data = pd.read_csv(\"datascience.csv\", index_col=0)\n",
    "scrape_data.shape\n",
    "#dropping any duplicate rows:\n",
    "scrape_data = scrape_data.drop_duplicates()\n",
    "scrape_data.reset_index(drop=True, inplace=True)\n",
    "scrape_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nothing_found    1506\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_data[\"salary\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some targeted cleaning of salary information to make parsing easier #—> remove “\\n”, “$”, and “,”\n",
    "scrape_data[\"salary\"] = scrape_data[\"salary\"].str.replace(\"\\n\", \"\")\n",
    "scrape_data[\"salary\"] = scrape_data[\"salary\"].str.replace(\",\", \"\")\n",
    "scrape_data[\"salary\"] = scrape_data[\"salary\"].str.replace(\"$\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_data[\"og_salary_period\"] = np.nan\n",
    "#if the salary contains information on time period, save that time\n",
    "#period string in the og_salary_period column\n",
    "scrape_data.ix[scrape_data[\"salary\"].str.contains(\"year\"), \"og_salary_period\"] = \"year\"\n",
    "scrape_data.ix[scrape_data[\"salary\"].str.contains(\"month\"), \"og_salary_period\"] = \"month\"\n",
    "scrape_data.ix[scrape_data[\"salary\"].str.contains(\"week\"), \"og_salary_period\"] = \"week\"\n",
    "scrape_data.ix[scrape_data[\"salary\"].str.contains(\"day\"), \"og_salary_period\"] = \"day\"\n",
    "scrape_data.ix[scrape_data[\"salary\"].str.contains(\"hour\"), \"og_salary_period\"] = \"hour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filtering out the results with salary data != \"Nothing_found\"\n",
    "salary_data = scrape_data[scrape_data[\"salary\"] != \"Nothing_found\"]\n",
    "#removing all rows in salary data from scrape data, and converting #all \"Nothing_found\" values to NaN, so that float salary values can #be easily reintegrated later\n",
    "scrape_data = scrape_data[~scrape_data.isin(salary_data)].dropna(how=\"all\")\n",
    "scrape_data[\"salary\"].replace(\"Nothing_found\",np.nan, inplace=True)\n",
    "scrape_data[\"salary\"].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separating out data based on whether contains hour, day, week, #month, year\n",
    "year_salaries = salary_data[salary_data[\"salary\"].str.contains(\"year\")]\n",
    "month_salaries = salary_data[salary_data[\"salary\"].str.contains(\"month\")]\n",
    "week_salaries = salary_data[salary_data[\"salary\"].str.contains(\"week\")]\n",
    "day_salaries = salary_data[salary_data[\"salary\"].str.contains(\"day\")]\n",
    "hour_salaries = salary_data[salary_data[\"salary\"].str.contains(\"hour\")]\n",
    "# removing string values(\" a year\", \" a week\", etc. from salary dfs)\n",
    "year_salaries[\"salary\"] = year_salaries[\"salary\"].str.replace(\" a year\", \"\")\n",
    "month_salaries[\"salary\"] = month_salaries[\"salary\"].str.replace(\" a month\", \"\")\n",
    "week_salaries[\"salary\"] = week_salaries[\"salary\"].str.replace(\" a week\", \"\")\n",
    "day_salaries[\"salary\"] = day_salaries[\"salary\"].str.replace(\" a day\", \"\")\n",
    "hour_salaries[\"salary\"] = hour_salaries[\"salary\"].str.replace(\" an hour\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sal(i):\n",
    " try:\n",
    "   splt = i.split(\" — \",1)\n",
    "   first = float(splt[0])\n",
    "   second = float(splt[1])\n",
    "   return (first + second)/2\n",
    " except:\n",
    "   return float(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_salaries[“salary”] = year_salaries[“salary”].apply(split_sal)\n",
    "month_salaries[“salary”] = month_salaries[“salary”].apply(split_sal)\n",
    "month_salaries[“salary”] = month_salaries[“salary”] * 12\n",
    "week_salaries[“salary”] = week_salaries[“salary”].apply(split_sal)\n",
    "week_salaries[“salary”] = week_salaries[“salary”] * 52\n",
    "day_salaries[“salary”] = day_salaries[“salary”].apply(split_sal)\n",
    "day_salaries[“salary”] = day_salaries[“salary”] * 260\n",
    "hour_salaries[“salary”] = hour_salaries[“salary”].apply(split_sal)\n",
    "hour_salaries[“salary”] = hour_salaries[“salary”] * 2080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rejoining salary data into main scrape_data df\n",
    "combined_salaries = pd.concat([year_salaries, month_salaries, week_salaries, day_salaries, hour_salaries], axis=0)\n",
    "scrape_data = pd.concat([scrape_data, combined_salaries], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#targeted replace of “r&d” in job titles and summaries with #“research development”, as I want to remove “&” \n",
    "#from listings as part of general clean-up\n",
    "scrape_data[“job_title”] = scrape_data[“job_title”].str.replace(“R&D”, “research development”)\n",
    "scrape_data[“summary”] = scrape_data[“summary”].str.replace(“R&D”, “research development”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating data character cleaning function, and applying to all #columns, also lowercasing all string data for ease of later nlp\n",
    "def data_clean(df, column):\n",
    "  cleaning_list = [“+”, “$”,”/”,”,”,”?”,”.”,”;”,”-”,”@”,”!”,”&”,”%”,”^”,”*”,”)”,”(“, “\\n”]\n",
    "  for item in cleaning_list:\n",
    "    df[column] = df[column].str.replace(item, “ “)\n",
    "    df[column] = map(str.lower, df[column])\n",
    "#can’t clean the salary column due to float values, and don’t need #to clean og_salary, so keeping out of the for loop\n",
    "for column in scrape_data.columns[0:len(scrape_data.columns)-2]:\n",
    "  data_clean(scrape_data, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#where location == “united states” I filled this in with the city #location name, otherwise it’ll be useless if more than one city has #location data of “united states”\n",
    "#using numpy is faster than using .replace\n",
    "scrape_data[‘location’] = np.where(scrape_data[‘location’] == “united states”, scrape_data[“city”], scrape_data[‘location’])\n",
    "#I’m also preemptively simplfying location data to include only city #and state, cutting data off at zip code - note: “ \\d” splits #information on the first numeric digit in the string.\n",
    "scrape_data[“location”] = scrape_data[“location”].str.split(‘ \\d’, expand=True, n=1)\n",
    "scrape_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_data.to_csv(“scraped_clean.csv”)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
